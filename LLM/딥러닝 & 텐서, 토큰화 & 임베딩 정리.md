
## # 📌 딥러닝 & 텐서, 토큰화 & 임베딩 정리

## 1️⃣ 딥러닝 (Deep Learning)
### ✅ 딥러닝의 개념
- 기존 머신러닝(Shallow Learning)과 달리 **특징(feature)을 사람이 직접 설계하지 않고** 데이터에서 자동으로 학습하는 방식이다.
- **역전파(Backpropagation)**를 통해 모델이 스스로 가중치를 조정하면서 최적의 성능을 찾아간다.
- 학습 과정:
  1. **순전파(Forward Propagation)** → 입력 데이터를 신경망을 통과시켜 예측값(`y_pred`)을 생성
  2. **손실 계산(Loss Calculation)** → 예측값과 실제값(`y_true`)의 차이를 계산하여 Loss 값 생성
  3. **역전파(Backpropagation)** → 오차를 기반으로 각 뉴런의 가중치를 업데이트
  4. **가중치 업데이트(Gradient Descent)** → 학습률(Learning Rate)을 기반으로 최적의 가중치를 찾음

---

## 2️⃣ 텐서 (Tensor)
### ✅ 텐서의 개념
- **텐서(Tensor)는 다차원 행렬(N-dimensional array)**이다.
- 스칼라(0D), 벡터(1D), 행렬(2D), 고차원 배열(3D~nD)까지 포함하는 개념이다.
- PyTorch, TensorFlow 등 딥러닝 프레임워크에서 데이터를 저장하고 연산하는 기본 구조이다.

### ✅ 텐서의 차원별 예시
| 차원  | 개념           | 예제                                     |
| --- | ------------ | -------------------------------------- |
| 0D  | 스칼라 (Scalar) | `x = 5`                                |
| 1D  | 벡터 (Vector)  | `[1, 2, 3]`                            |
| 2D  | 행렬 (Matrix)  | `[[1, 2], [3, 4]]`                     |
| 3D+ | 고차원 텐서       | `[[[1, 2], [3, 4]], [[5, 6], [7, 8]]]` |

### ✅ 텐서와 넘파이(NumPy) 배열의 차이점
| 비교 | NumPy 배열 | 텐서(Tensor) |
|------|-------------|--------------|
| **연산 지원** | CPU에서만 동작 | GPU/TPU에서도 동작 |
| **자동 미분(Backpropagation)** | 지원 안됨 | 지원됨 (PyTorch, TensorFlow) |
| **다차원 연산** | 가능 | 가능 (더 최적화됨) |

---

## 3️⃣ 토큰화 (Tokenization)
### ✅ 토큰과 토크나이저의 개념
- **토큰(Token)**: 자연어(NL)에서 단어를 나누는 단위
- **토크나이저(Tokenizer)**: 텍스트를 토큰으로 변환하는 방법
- 언어별, 특성별로 다양한 토크나이저 모델이 존재함.

## 4️⃣ 임베딩 (Embedding)

### ✅ 임베딩의 개념

- **임베딩(Embedding)은 토큰(Token)을 n차원 행렬(벡터)로 변환하는 과정이다.**
- 단어 간의 의미적 관계를 반영하기 위해 사용된다.
- 원-핫 인코딩(One-Hot Encoding)의 한계를 해결하는 방식이다.

### ✅ 원-핫 인코딩 vs 임베딩 비교

| 방법      | 설명                                   | 단점               |
| ------- | ------------------------------------ | ---------------- |
| 원-핫 인코딩 | 각 단어를 고유한 벡터로 표현 (`[1, 0, 0]`)       | 의미 반영 불가능, 차원 커짐 |
| 임베딩     | 단어 간의 관계를 반영한 벡터 (`[0.8, 0.2, 0.1]`) | 사전 학습 필요         |

### ✅ 임베딩 벡터 예시

|단어|원-핫 벡터|임베딩 벡터 (예제)|
|---|---|---|
|사과|`[1, 0, 0, 0]`|`[0.8, 0.2, 0.1]`|
|바나나|`[0, 1, 0, 0]`|`[0.7, 0.3, 0.2]`|
|강아지|`[0, 0, 1, 0]`|`[0.1, 0.9, 0.8]`|
|고양이|`[0, 0, 0, 1]`|`[0.2, 0.8, 0.9]`|

### ✅ 임베딩의 장점

1. **의미 기반 표현 가능** → 단어 간 유사성을 반영할 수 있음.
2. **차원 축소** → 원-핫 벡터보다 메모리 효율적.
3. **문장, 문서까지 확장 가능** → 검색, 추천, 챗봇 등에 활용 가능.
4. **벡터 연산이 가능함** → `"왕 - 남자 + 여자 = 여왕"` 같은 연산 수행 가능.

### ✅ 임베딩 모델의 종류

|모델|특징|예제|
|---|---|---|
|Word2Vec|주변 단어를 이용해 벡터를 학습|`Gensim`|
|FastText|문자 단위 정보까지 반영|`Facebook AI`|
|BERT|문맥을 반영하는 딥러닝 기반 임베딩|`Hugging Face`|
|OpenAI Embeddings|최신 GPT 모델을 기반으로 학습|`text-embedding-ada-002`|





