
# 📌 LLM 개념과 사용법 정리

## 4️⃣ LLM과 활용 방식
### 🔹 LLM = 트랜스포머 모델의 확장 버전?
- **LLM은 트랜스포머 모델의 부분집합!**
- **트랜스포머 모델을 기반으로 사전 학습(Pretraining) + 추가적인 데이터 학습(Fine-tuning) → LLM 완성**
- ✅ `GPT-4`, `LLaMA`, `Claude`, `Mistral` 등

### 🔹 LLM 활용 방식
1. **API 호출 방식:** OpenAI, Hugging Face 등에서 제공하는 LLM을 API로 호출  
2. **로컬 실행 방식:** `LLaMA`, `Mistral` 같은 모델을 직접 다운로드하여 실행  
3. **RAG 방식:** 벡터DB와 결합하여, LLM이 직접 학습하지 않은 정보를 검색 후 답변 생성  

---

## 5️⃣ RAG (검색 증강 생성, Retrieval-Augmented Generation)
### 🔹 RAG 개념
- LLM이 모든 정보를 기억하는 것이 아니라, **외부 데이터베이스(벡터DB)에서 검색한 정보를 기반으로 응답을 생성**하는 방식.
- RAG는 "프롬프트 엔지니어링 + 검색 시스템"이 결합된 개념

### 🔹 RAG의 한계
- ✅ 일반적인 문서는 효과적이지만, **특수한 도메인에서는 한계** 존재.
- ✅ 검색된 청크(문서 조각)가 짧으면 **정보 부족 → 신뢰성 저하**.

### 🔹 RAG 최적화: **"Semantic B-Tree"**
- 검색된 문서를 바로 사용하지 않고, **요약 및 청크 확장(Chunk Expansion) 기법 적용**.
- ✅ **요약 (Summarization):** 검색된 정보의 밀도를 높여서 더 적은 토큰으로 유의미한 답변 생성.
- ✅ **청크 확장 (Chunk Expansion):** 검색된 청크의 앞뒤 문맥을 추가하여 정보 부족 문제 해결.
- ✅ **정보 필터링:** 검색된 문서 중 정보 밀도가 높은 것만 선택하여 LLM에 입력.

---

## 6️⃣ 로컬 모델 vs OpenAI API vs Fine-tuning 조합 비교
| 조합 | 특징 | 장점 | 단점 |
|------|------|------|------|
| **로컬모델 + 파인튜닝** | 특정 도메인에 맞게 모델 자체를 학습하여 맞춤형 응답 생성 가능 | ✅ 특정 도메인에 최적화된 모델<br>✅ API 비용 없이 로컬 실행 가능<br>✅ 보안 문제 최소화 (데이터 유출 없음) | 🚨 학습 데이터 구축 필요<br>🚨 모델 훈련 비용 및 GPU 자원 필요<br>🚨 **토큰 제한 존재 (VRAM 한계, 컨텍스트 윈도우 제한)** |
| **로컬모델 + RAG (Semantic B-Tree 포함)** | 사전 학습된 로컬 모델을 사용하고, 벡터DB를 활용하여 검색 증강 | ✅ 최신 데이터 반영 가능<br>✅ 검색 기반 최적화로 모델 업데이트 필요 없음<br>✅ 특정 도메인에서도 신뢰성 높은 응답 생성 가능 | 🚨 벡터DB 구축 및 유지보수 필요<br>🚨 로컬 모델이 OpenAI 대비 성능이 낮을 가능성<br>🚨 **토큰 제한 존재 (VRAM 한계, 컨텍스트 윈도우 제한)** |
| **OpenAI + RAG (Semantic B-Tree 포함)** | OpenAI의 강력한 LLM을 활용하면서 RAG를 추가하여 성능 최적화 | ✅ OpenAI의 강력한 성능 활용 가능<br>✅ 벡터DB를 활용한 검색 최적화 가능<br>✅ 모델 학습 불필요, 즉시 사용 가능 | 🚨 OpenAI API 비용 발생<br>🚨 데이터 보안 문제 존재 (서버로 데이터 전송 필요)<br>🚨 **토큰 제한 존재 (OpenAI의 API 제한, 가격 부담)** |

---

## 7️⃣ LLM 최적화 전략 (토큰 제한 회피)
### 🔹 1️⃣ 검색된 문서를 "요약"하여 정보 밀도를 높이기
- ✅ 긴 문서를 그대로 LLM에 입력하면 토큰이 너무 많아짐.
- ✅ 미리 요약(Summarization)하여 **"최소한의 토큰으로 최대한의 정보"를 포함**.

### 🔹 2️⃣ "청크 확장(Chunk Expansion)"을 통해 문맥 추가
- ✅ 검색된 문서의 앞뒤 문맥까지 포함하여 정보 부족 문제 해결.

### 🔹 3️⃣ Fine-tuning 활용 (로컬에서 적용 가능)
- ✅ 특정 도메인에서는 검색 기반 접근(RAG)보다 **Fine-tuning을 통해 LLM을 학습시키는 것이 더 효과적**.

---

# **✅ 결론**
✔ **LLM은 트랜스포머 기반이며, API 호출 or 로컬 실행 방식으로 활용 가능.**  
✔ **토큰 제한 문제를 해결하려면 "Semantic B-Tree" 같은 검색 최적화 기법이 필요.**  
✔ **Fine-tuning과 RAG는 상호보완적이며, 특수 도메인에서는 RAG만으로 해결하기 어렵다.**  
✔ **로컬 모델 vs OpenAI API vs Fine-tuning 각각의 장단점을 고려하여 선택해야 한다.**  