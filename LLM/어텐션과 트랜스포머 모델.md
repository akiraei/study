# 📌 어텐션(Attention), 셀프 어텐션(Self-Attention), 멀티 헤드 어텐션(Multi-Head Attention), 트랜스포머(Transformer) 모델 정리

## **1️⃣ 어텐션(Attention)**
### ✅ 어텐션이란?
- **문장에서 중요한 단어에 더 집중하도록 도와주는 기술**이다.
- 기존 RNN/LSTM의 한계를 해결하기 위해 등장했다.

### ✅ 기존 RNN/LSTM의 한계
| 한계점 | 설명 |
|--------|------|
| **장기 의존성 문제(Long-Term Dependency)** | 문장이 길어질수록 앞의 단어를 잘 기억하지 못함. |
| **병렬 처리 불가능** | RNN/LSTM은 단어를 순차적으로 처리해야 해서 학습 속도가 느림. |

### ✅ 어텐션의 원리
- 입력 문장에서 각 단어가 다른 단어와 얼마나 관련이 있는지 가중치를 계산한다.
- **쿼리(Query, Q), 키(Key, K), 값(Value, V)**를 사용하여 관계를 측정한다.

| 용어 | 설명 |
|------|------|
| **쿼리(Query, Q)** | 현재 단어가 다른 단어들과 얼마나 관련이 있는지 확인하는 벡터 |
| **키(Key, K)** | 문장 내의 각 단어가 가지고 있는 특징을 나타내는 벡터 |
| **값(Value, V)** | 단어의 정보를 포함한 벡터 |
| **어텐션 스코어(가중치, Weight)** | 특정 단어가 현재 단어와 얼마나 중요한지를 나타내는 점수 |

### ✅ 어텐션 적용 예제 (영어 → 한국어 번역)
📌 **입력 문장:** `"I love you"`  
📌 **출력 문장:** `"나는 너를 사랑해"`

✅ `"I"`와 `"나는"`, `"love"`와 `"사랑해"`의 관계를 분석하여 적절한 번역을 생성함.  
✅ **즉, 어텐션은 서로 다른 입력과 출력 간의 관계를 학습하는 기술!** 🚀

---

## **2️⃣ 셀프 어텐션(Self-Attention)**
### ✅ 셀프 어텐션이란?
- **같은 문장 내에서 단어들이 서로를 참고하는 방식이다.**
- 문장에서 중요한 단어를 강조하고, 불필요한 단어를 걸러냄.

### ✅ 셀프 어텐션 적용 예제
📌 **입력 문장:** `"나는 커피를 마셨다."`  
✅ `"마셨다"`는 `"커피"`와 강한 연관이 있으므로 높은 가중치를 받음.  
✅ 반면 `"나는"` 같은 단어는 낮은 가중치를 가짐.  

### ✅ 어텐션(Attention) vs 셀프 어텐션(Self-Attention)
| 개념 | 설명 | 사용 예제 |
|------|------|----------|
| **어텐션(Attention)** | 서로 다른 입력과 출력 간의 관계를 학습 | 영어 → 한국어 번역 |
| **셀프 어텐션(Self-Attention)** | 같은 문장에서 단어들 간의 관계를 학습 | 문맥 이해 (GPT, BERT) |

---

## **3️⃣ 멀티 헤드 어텐션(Multi-Head Attention)**
### ✅ 멀티 헤드 어텐션이란?
- 여러 개의 어텐션을 병렬로 수행한 후, 이를 선형 결합(Concatenation)하는 방식이다.

### ✅ 왜 필요한가?
✅ 기본 어텐션은 **한 가지 패턴만 학습**하지만,  
✅ 멀티 헤드 어텐션은 **여러 가지 관점에서 단어 간의 관계를 분석할 수 있음!**

예를 들어:
1. **1번 어텐션:** 단어의 **의미적 관계** 학습
2. **2번 어텐션:** 단어의 **문법적 관계** 학습
3. **3번 어텐션:** 단어의 **위치 정보** 학습

📌 **즉, 멀티 헤드 어텐션은 어텐션(혹은 셀프 어텐션)의 선형 결합이라고 볼 수 있음!** 🚀  

---

## **4️⃣ 트랜스포머(Transformer) 모델**
### ✅ 트랜스포머란?
- **어텐션을 기반으로 한 신경망 모델**이다.
- 기존 RNN/LSTM의 한계를 극복하며, 빠르고 효율적으로 문장을 처리할 수 있다.

### ✅ 트랜스포머의 주요 구조
1. **인코더(Encoder)** → 입력 문장을 벡터로 변환  
2. **디코더(Decoder)** → 벡터를 다시 문장으로 변환  

✅ **대표적인 트랜스포머 모델들**
| 모델 | 특징 |
|------|----------------|
| **BERT** | 인코더 기반, 문장 의미 이해 |
| **GPT** | 디코더 기반, 문장 생성 (GPT-4, ChatGPT) |
| **T5** | 인코더 + 디코더 조합 |

📌 **예제: ChatGPT의 동작 방식**
1. 문장을 입력하면 → "토큰화"
2. "어텐션"을 이용해 문맥 파악
3. GPT(디코더)가 답변 생성

---

## **🚀 결론**
✔ **어텐션(Attention)** = 서로 다른 입력 간의 관계를 찾음.  
✔ **셀프 어텐션(Self-Attention)** = 같은 입력 내에서 단어 간의 관계를 찾음.  
✔ **멀티 헤드 어텐션(Multi-Head Attention)** = 여러 개의 어텐션을 병렬로 수행하여 다양한 패턴을 학습.  
✔ **트랜스포머(Transformer)** = 어텐션을 기반으로 동작하며, GPT와 BERT 같은 모델의 핵심 원리.  