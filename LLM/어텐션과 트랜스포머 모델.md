# 📌 어텐션(Attention)과 트랜스포머(Transformer) 모델 정리

## **1️⃣ 어텐션(Attention)이란?**

💡 **핵심 개념:**

- **어텐션(Attention)은 문장에서 중요한 단어를 자동으로 찾아내서 더 집중하는 기술**이다.
- RNN, LSTM과 같은 기존 모델이 **긴 문장에서 중요한 단어를 기억하지 못하는 문제**를 해결하기 위해 등장했다.

### ✅ 기존 RNN/LSTM의 한계

|한계점|설명|
|---|---|
|**장기 의존성 문제(Long-Term Dependency)**|문장이 길어질수록 앞의 단어를 잘 기억하지 못함.|
|**병렬 처리 불가능**|RNN/LSTM은 단어를 **순차적으로 처리**해야 해서 학습 속도가 느림.|

💡 **➡ 이를 해결하기 위해 나온 것이 '어텐션'!**

---

## **2️⃣ 어텐션의 원리**

**어텐션은 문장 내 단어들 간의 관계를 실시간으로 측정하는 기술이다.**  
즉, **어떤 단어가 현재 단어와 얼마나 중요한지를 가중치(Attention Score)로 계산**한다.

### ✅ 어텐션의 핵심 요소

1️⃣ **쿼리(Query, Q)** → "현재 보고 있는 단어"  
2️⃣ **키(Key, K)** → "참조할 단어"  
3️⃣ **값(Value, V)** → "참조한 단어의 정보"  
4️⃣ **어텐션 스코어(가중치, Weight)** → "어떤 단어가 중요한지 판단"

📌 **예제: "나는 커피를 마셨다."**

- `Q = "마셨다"` (현재 단어)
- `K = ["나는", "커피", "를", "마셨다"]` (참조할 모든 단어)
- `"커피"`가 "마셨다"와 강한 연관이 있다고 판단하면 **높은 가중치**를 줌.

✅ **어텐션을 통해 문장에서 중요한 단어를 자동으로 찾아내서 집중할 수 있음.**  
✅ **멀리 떨어진 단어와의 관계도 파악 가능!**

---

## **3️⃣ 임베딩(Embedding)과 어텐션의 차이**

**둘 다 '관계성'을 측정하지만, 적용 방식이 다름.**

|개념|역할|관계 유형|적용 방식|
|---|---|---|---|
|**임베딩(Embedding)**|단어를 벡터로 변환|**일반적인 관계** (문맥과 무관)|사전 학습된 모델 (Word2Vec, BERT 등)|
|**어텐션(Attention)**|문맥에서 중요한 단어 찾기|**문맥 기반 관계** (동적)|입력된 문장에 따라 실시간 계산|

### ✅ 예제 비교

📌 **임베딩:** `"고양이"`와 `"개"`의 관계는 항상 비슷한 벡터를 가짐.  
📌 **어텐션:** `"강아지"`와 `"놀았다"`의 관계는 문맥에 따라 다름.

✅ **즉, 임베딩은 미리 학습된 단어 간의 '고정된' 관계이고, 어텐션은 입력된 문장에서 '실시간으로' 관계를 분석!**

---

## **4️⃣ 트랜스포머(Transformer)란?**

💡 **트랜스포머는 어텐션을 기반으로 한 신경망 모델이다.**

### ✅ 기존 RNN/LSTM과의 차이점

|모델|특징|장점|
|---|---|---|
|**RNN**|단어를 순차적으로 처리|속도가 느림, 장기 의존성 문제|
|**LSTM**|기억을 보완한 RNN|여전히 순차적, 병렬 처리 불가|
|**Transformer**|모든 단어를 동시에 처리 (병렬)|빠르고, 문맥을 더 잘 파악|

✅ **즉, 트랜스포머는 '어텐션'을 이용해 병렬 연산이 가능하고, 긴 문장도 쉽게 학습할 수 있다.**

---

## **5️⃣ 트랜스포머 모델 구조**

트랜스포머는 크게 **인코더(Encoder)와 디코더(Decoder)**로 구성된다.

### ✅ 트랜스포머의 주요 구조

1️⃣ **인코더(Encoder)** → 입력 문장을 벡터로 변환  
2️⃣ **디코더(Decoder)** → 벡터를 다시 문장으로 변환

✅ **대표적인 트랜스포머 모델들**

|모델|특징|
|---|---|
|**BERT**|인코더 기반, 문장 의미 이해|
|**GPT**|디코더 기반, 문장 생성 (GPT-4, ChatGPT)|
|**T5**|인코더 + 디코더 조합|

📌 **예제: ChatGPT의 동작 방식**

1. 문장을 입력하면 → "토큰화"
2. "어텐션"을 이용해 문맥 파악
3. GPT(디코더)가 답변 생성

---

## **🚀 결론**

✔ **어텐션(Attention)은 문장에서 중요한 단어를 찾는 기술이다.**  
✔ **트랜스포머(Transformer)는 어텐션을 기반으로 만들어진 딥러닝 모델이다.**  
✔ **GPT는 트랜스포머의 디코더 구조를 사용하여 텍스트를 생성한다.**  
✔ **임베딩은 단어 간 '일반적인' 관계를 반영하고, 어텐션은 '문맥 기반' 관계를 실시간으로 반영한다.**